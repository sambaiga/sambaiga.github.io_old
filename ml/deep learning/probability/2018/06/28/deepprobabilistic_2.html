<!DOCTYPE html>
<html lang="en">

  
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <title>Bayesian Inference | 
  sambaiga
</title>
  

  
  <meta name="description" content="The post introduce the basics principle of bayesian inference
">
  
 
  
  <link rel="stylesheet" href="/assets/css/style.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  
  <link rel="canonical" href="http://localhost:4000/ml/deep%20learning/probability/2018/06/28/deepprobabilistic_2.html">
  <link rel="alternate" type="application/rss+xml" title="
  sambaiga
" href="/feed.xml">

  
  
  <meta name="theme-color" content="#ffffff">

  

  

  

  

  

<script src="https://use.fontawesome.com/3652afeffa.js"></script>
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
  <!-- Load jQuery -->
  <script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<script src="/assets/js/common.js"></script>
</head>



  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <div class="site-header-float">
    



<a class="site-title" href="




/blog/
">
  
    
  sambaiga

  
</a>

    

    
    <nav class="site-nav">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </span>

      <div class="trigger">
        

<span class="nav-list-title">Menu:</span>
<ul class="nav-list ">






  



  
  <li><a class="page-link" href="/about/">About</a></li>
  



  
  <li><a class="page-link" href="/blog/">Blog</a></li>
  



  



  



  



  
  <li><a class="page-link" href="/project/">Projects</a></li>
  



  
  <li><a class="page-link" href="/resources.html">Resources</a></li>
  



  
  <li><a class="page-link" href="/talks/">Talk</a></li>
  



  



  



  


</ul>

      </div>
    </nav>
    
    </div>

    
    

  </div>

</header>


    
    <main class="post-content" aria-label="Content">
    
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  
  <header class="post-header with-thumbnail">
  
    
    <img class="post-thumbnail" src="/assets/img/post/probability.jpg" alt="Bayesian Inference"/>
    
    <div class="wrapper">
      <h1 class="post-title" itemprop="name headline">Bayesian Inference</h1>
      <p class="post-meta">
        <time datetime="2018-06-28T17:12:00+02:00" itemprop="datePublished">
          

  Jun 28, 2018


        </time>
        





  
  

  
    <span class="last-update">·

    

    last updated on
    

  Jul 26, 2018



    

    </span>
  




        
      </p>
      <h3 class="post-summary">The post introduce the basics principle of bayesian inference
</h3>
    </div>
  </header>

  <div class="post-content" itemprop="articleBody">
    <h2 id="1-introduction">1. Introduction</h2>

<p>Bayesian method offer a different paradigm for doing statistical analysis. It is practical method for making inferences from data using probability models. Unlike other statistical approach, bayesian models are easy to interpret and incorporate uncertainty. In bayesian method  we start with a belief which is also called a <strong>prior</strong>. We then update our belief after observing some data. The outcome is called a <strong>posterior</strong>. The process repeats as we keep on observing more data where the old <em>posterior</em> becomes a new <em>prior</em>. The  process employs the Bayes rule.</p>

<p>Consider the Bayesian theorem, which allows us to use some knowledge or belief that we already have. Given data point <script type="math/tex">\mathcal{D} = \{x \in \mathbb{R}^{N\times d}, y \in \mathbb{R}^{N\times c}\}</script>. The Bayesian  approach  treat the latent variable or parameter <script type="math/tex">z</script> as random variable with some prior distribution <script type="math/tex">p(z)</script>. This is the probability of parameters <script type="math/tex">z</script> before hand.</p>

<script type="math/tex; mode=display">p(z | \mathcal{D}  ) = \frac{p(\mathcal{D} | z )\cdot p(z)}{p(\mathcal{D})}</script>

<p>where</p>

<script type="math/tex; mode=display">p(\mathcal{D}) = \int p(\mathcal{D} |z )\cdot p(z) dz</script>

<p>From the bayesian theorem above,  <script type="math/tex">z</script> is the hypothesis about the world, and <script type="math/tex">\mathcal{D}</script> is the data or evidence. The probability <script type="math/tex">p(\mathcal{D} \mid z)</script> is called  <strong>likeli-hood</strong>; the probability of data given the latent variable and <script type="math/tex">p(\mathcal{D})</script> is the <strong>marginal-likelihood</strong> and <script type="math/tex">p(z \mid \mathcal{D}  )</script> is the <strong>posterior</strong>.</p>

<h3 id="11-bayesian-inference">1.1 Bayesian Inference</h3>

<p>Given data set <script type="math/tex">\mathcal{D}</script> and latent variable <script type="math/tex">z</script> that relate <script type="math/tex">x</script> and <script type="math/tex">y</script> such that:
<script type="math/tex">y = f_{z}(x:z)</script>
The first step in bayesian inference is to identify the parameter <script type="math/tex">z</script> and express our lack of knowledge  about this parameter in term of probability distribution <script type="math/tex">p(z)</script>. This is the prior knowledge about the parameter <script type="math/tex">z</script>. After that we express a <em>likelihood</em>  <script type="math/tex">p(\mathcal{D} \mid z)</script> which tell us how the data <script type="math/tex">\mathcal{D}</script> interact with  parameter <script type="math/tex">z</script>. Together the prior and the likelihood make our model (generative model). It tell us how we can simulate from our data.</p>

<p>In <strong>training</strong> stage we apply Bayesian theorem to get posterior distribution:</p>

<script type="math/tex; mode=display">p(z|\mathcal{D}) = \frac{p(\mathcal{D}|, z)}{p(\mathcal{D})}</script>

<p>In testing stage we find <strong>predictive-distribution</strong></p>

<script type="math/tex; mode=display">p(\hat{y}| x, \mathcal{D}) = \int p(\hat{y} | x, z)\cdot p(z | x, y) dz</script>

<p>Theoretically this is how bayesian inference work, however the big challenge is how to compute the posterior distribution. The integral <script type="math/tex">\int p(\mathcal{D} \mid z )\cdot p(z) dz</script> is usually intractable. Therefore we need to approximate this integral. There several approach used for approximation. The first and simple approach is to use <em>analytical integration</em> which is only applicable with the use of <a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate prior</a> and thus possible only for very simple cases. However the approach does not scale well. Alternatively <em><a href="https://metacademy.org/graphs/concepts/laplace_approximation">Gaussian (Laplace) approximation</a></em> can be applied where the posterior is  approximated  with Gaussian. This works well when there are lot of data. <a href="https://metacademy.org/graphs/concepts/markov_chain_monte_carlo">Markov Chain Monte Carlo (MCMC)</a> are among other popular techniques used for approximating the integral. The method is applicable to wide variety of problems however is very slow.</p>

<p>The popular and recently widely used approach for approximating prior in bayesian method is <em>Variational Inference</em>. This approach convert the integral into optimization problem and thus Work faster than MCMC. In this post we will introduce the basic concepts behind variational inference and different algorithms for performing variation inferences. For dive into varitiaonl infrence let us revisit one example of bayesian inference in regression problem.</p>

<h3 id="12-bayesian-linear-regression">1.2 Bayesian Linear Regression</h3>
<p>Given data point <script type="math/tex">X = \{x_1,\ldots x_N \}</script> and corresponding target <script type="math/tex">Y = \{y_1,\ldots y_N \}</script>. Linear regression assume that data is generated from the following model:</p>

<p><script type="math/tex">y= f_{\theta}(x: \theta) + \in</script>
 where <script type="math/tex">\in</script> is the noise due to measurement and 
<script type="math/tex">f_{\theta}(X: \theta)</script> is the hypothesis function given;</p>

<script type="math/tex; mode=display">f_{\theta}(X: \theta) = b + \sum_{i=1}^N w_i \phi (x_i) = \theta^T\cdot \phi(X)</script>

<p>where <script type="math/tex">\phi(X)</script> is the basis function and <script type="math/tex">\theta</script> is the model parameters such that 
<script type="math/tex">\theta_{0} =b</script>  and <script type="math/tex">\phi_0=1</script></p>

<p>The  output of this model is the single point estimate for the best model parameter. The Bayesian modelling approach to this problem offer a systematic framework for learning distribution over values of the parameters and not a single estimate. The bayesian linear regression model <script type="math/tex">y= f_{\theta}(x: \theta) + \in</script> as a Gaussian  distribution such that:</p>

<script type="math/tex; mode=display">p(y|x, \theta) = \mathcal{N}(y|f_{\theta}(x: \theta), \beta^{-1})</script>

<p>Assuming the data point are drawn independently and identically distributed the likelihood is expressed as:</p>

<script type="math/tex; mode=display">p(Y| X, \theta) = \prod_{i=1}^N \mathcal{N}(y_i|f_{\theta}(x_i: \theta_i), \beta^{-1})</script>

<p>Let choose a prior that is conjugate to the likelihood</p>

<script type="math/tex; mode=display">p(\theta|X) = \mathcal{N}(\theta|0, \alpha^{-1})</script>

<p>Thus the posterior is given as:</p>

<script type="math/tex; mode=display">p(\theta|Y, X) \propto \mathcal{N}(\theta|0, \alpha^{-1})\cdot \prod_{i=1}^N \mathcal{N}(y_i|f_{\theta}(x_i: \theta_i), \beta^{-1})</script>

<h2 id="2-variational-inference">2. Variational Inference</h2>
<p>In the previous section we show that inference in probabilist model is often intractable and introduced several approach used to approximate the inference. Variational Inference (VI) is one of the approach used to approximate difficult probability distribution by turning the calculation about model into optimization.</p>

<p>Consider a probabilistic model which is joint distribution <script type="math/tex">p(x,z)</script> of the latent variable  <script type="math/tex">z</script> observed variables <script type="math/tex">x</script>. To draw inference on the latent variable <script type="math/tex">z</script> we compute the posterior</p>

<script type="math/tex; mode=display">p(z|x) = \frac{p(x,z)}{p(x)} = \frac{p(x|z)\cdot p(z)}{p(x)}</script>

<p>where</p>

<script type="math/tex; mode=display">p(x)=\int p(x|z)\cdot p(z) dz</script>

<p>To approximate <script type="math/tex">p(z\mid x)</script> we first choose an approximating family of distribution <script type="math/tex">q(z)</script> over latent variable  <script type="math/tex">z</script>. Then we find set of parameters that makes <script type="math/tex">q(z)</script> close to posterior distribution <script type="math/tex">p(z\mid x)</script>. Thus VI approximate <script type="math/tex">p(z\mid x)</script> with new distribution <script type="math/tex">q(z)</script> such that <script type="math/tex">q(z)</script> is close to <script type="math/tex">p(z\mid x)</script>. To achieve this we minimize KL divergence between <script type="math/tex">q(z)</script> and <script type="math/tex">p(z\mid x)</script> such that:</p>

<script type="math/tex; mode=display">q^*(z) = \arg \min D_{KL}(q(z)||p(z|x))</script>

<p>where</p>

<script type="math/tex; mode=display">D_{KL}(q(z)||p(z|x)) = \int q(z)\log \frac{q(z)}{p(z|x)}</script>

<p>It clear that we can not minimize KL divergence since it is directly depend on posterior <script type="math/tex">p(z\mid x)</script>. However we can minimize a function that is equal to KL divergence plus constant. This function is called <strong>Evidence Lower Bound</strong>(ELBO) <script type="math/tex">\mathcal{L}_{VI}(q)</script>.</p>

<h3 id="21-evidence-lower-bound-elbo">2.1 Evidence Lower Bound (ELBO)</h3>

<p>To derive the ELBO we first consider the <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen’s inequality</a> which relates the value of a convex function of an integral to the integral of the convex function such that <script type="math/tex">f(\mathbb{E}[x]) \geq \mathbb{E}[f(x)]</script> where <script type="math/tex">f(.)</script> is the concave function. Since logarithmic are strictly concave function it is clear that</p>

<script type="math/tex; mode=display">\log \int p(x)g(x) dx \geq \int p(x)\log g(x)</script>

<p>Let us consider a log of marginal evidence.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} 
\log p(x) & = \log \int_z p(x,z) dz\\
          & =\log \int_z p(x,z)\cdot \frac{q(z)}{q(z)} dz \\
          & =\log \int_z q(z)\frac{p(x,z)}{q(z)} dz\\
          & =\log \left(\mathbb{E}_q[\frac{p(x,z)}{q(z)}] \right)\\
          &\geq \mathbb{E}_q[ \log p(x,z)] - \mathbb{E}_q[\log q(z)]
\end{aligned} %]]></script>

<p>The final line is the ELBO which is the lower bound for the evidence. Thus the evidence lower bound for probability model <script type="math/tex">p(x,z)</script> and approximation <script type="math/tex">q(z)</script> to the posterior is</p>

<script type="math/tex; mode=display">\mathcal{L}_{VI}(q) = \mathbb{E}_q[ \log p(x,z)] - \mathbb{E}_q[\log q(z)]</script>

<p>We can now show that KL divergence to the posterior is equal to the negative ELBO plus constant.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} 
D_{KL}(q(z)||p(z|x)) &= \int q(z)\log \frac{q(z)}{p(z|x)}\\
                     &= \mathbb{E}_q[\log q(z)] - \mathbb{E}_q[\log p(x,z)] + \mathbb{E}_q[\log p(x)]\\
                     &=-\left(\mathbb{E}_q[\log p(x,z)] - \mathbb{E}_q[\log q(z)] \right) +\log p(x)\\
                     &= -\mathcal{L}_{VI}(q) +\log p(x)\\
\mathcal{L}_{VI}(q) &=\log p(x) + D_{KL}(q(z)||p(z|x))
\end{aligned} %]]></script>

<p>From the equation above it clear that minimizing the KL divergence is equivalent to maximizing the ELBO. Recall that we want to find <script type="math/tex">q(z)</script> such that KL divergence is small, the variational objective function becomes</p>

<script type="math/tex; mode=display">q^*(z) = \arg \min D_{KL}(q(z)||p(z|x)) = \arg \max \mathcal{L}_{VI}(q)</script>

<h3 id="3-mean-field-variational-inference">3 Mean Field Variational Inference</h3>

<p>One of the important question on VI, is how to construct the family of variational distributions from which we want to draw <script type="math/tex">q(z)</script> from. The simplest family is where each latent parameter <script type="math/tex">z_i</script> has its own
independent distribution. This means that we can easily factorize the variational distributions into groups:</p>

<script type="math/tex; mode=display">q(z_1, \ldots, z_m) = \prod_{i=1}^m q(z_i)</script>

<p>This family of distribuion are known as Mean-Field Variational Family that make use of <a href="https://en.wikipedia.org/wiki/Mean_field_theory">mean field theory</a>. Inference using this factorization is known as Mean-Field Variational Inference (MFVI).</p>

<p>It possible to further parameterize the approximating distributions <script type="math/tex">q(z)</script> with variational parameters <script type="math/tex">\lambda</script> such that the approximating distribution become <script type="math/tex">q(z_i ; \lambda_i)</script>. For example if we set our family of approximating distributions as a set of
independent gauasian distributions <script type="math/tex">\mathcal{N}(\mu_i, \sigma^2_i)</script> and parameterize this distributions with the mean and variance where <script type="math/tex">\lambda_i = (\mu_i, \sigma^2_i)</script> is the set of variational parameters.</p>

<p>The common algorithms used in practise to do VI under mean filed assumptions are coordinate ascent optimization (CAVI) and stochastic gradient based method.</p>

<h3 id="31-coordinate-ascent-variational-inference-cavi">3.1 Coordinate Ascent Variational Inference (CAVI)</h3>

<p>The CAVI algorithm derive variational updates by hand and perform coordinate ascent (iteratively updating each latent variable <script type="math/tex">z_i</script> ) on the latent until convergence of the ELBO. A common procedure to conduct CAVI is:</p>

<ul>
  <li>Choose variational distributions <script type="math/tex">q(z)</script></li>
  <li>Compute ELBO;</li>
  <li>Optimize individual <script type="math/tex">q(z_i)</script> ’s by taking the gradient for each latent variable;</li>
  <li>Repeat until ELBO converges.</li>
</ul>

<p>The coordinate ascent update for a latent variable can be derived by maximizing the ELBO function above. First, recall ELBO</p>

<script type="math/tex; mode=display">\mathcal{L}_{VI}(q) = \mathbb{E}_q[ \log p(x,z)] - \mathbb{E}_q[\log q(z)]</script>

<p>Using chain rule we can decomopse the joint  <script type="math/tex">p(x,z)</script> as;</p>

<script type="math/tex; mode=display">p(x_{1:n}, z_{1:m}) = p(x_{1:n}) \prod_{i=1}^m p(z_i|z_{1:(i-1)}, x_{1:n})</script>

<p>Using mean field approximation, we can decompose the entropy term of the ELBO as</p>

<script type="math/tex; mode=display">\mathbb{E}_q[\log q(z)] = \sum_{i=1}^m \mathbb{E}_q[\log q(z_i)]</script>

<p>Under the above assumption the ELBO become:</p>

<script type="math/tex; mode=display">\mathcal{L}_{ELBO}(q) = \log p(x_{1:n}) + \sum_{i=1}^m \mathbb{E}_q[\log p(z_i|z_{1:(i-1)}, x_{1:n}) ] - \mathbb{E}_q[\log q(z_i)</script>

<p>To find this <script type="math/tex">\arg \max_{q(z_i)} \mathcal{L}_{ELBO}(q)</script> we take derivative of ELBO with respect to <script type="math/tex">q(z_i)</script> using Lagrange multipliers and set the derivative to zero. It can be shown that the coordinate ascent update rule is equal to</p>

<script type="math/tex; mode=display">q^*(z_i) \propto \{  \mathbb{E}_{q-i}[\log q(z_i,z_{\neg},x)]\}</script>

<p>where the notation <script type="math/tex">\neg</script> denotes all indices other than the <script type="math/tex">i^{th}</script></p>

<p>Despite being very fast  method for some models  only  work with  conditionally conjugate models.</p>

<h3 id="reference">Reference</h3>

<ul>
  <li><a href="http://www.tamarabroderick.com/tutorial_2018_icml.html">ICML 2018 tutorial</a>:Variational Bayes and Beyond: Bayesian Inference for Big Data.</li>
  <li><a href="https://www.shakirm.com/papers/VITutorial.pdf">Shakir Mohamed</a>:Variational Inference  for Machine Learning.</li>
  <li><a href="https://emtiyaz.github.io/teaching/ds3_2018/ds3.html">DS3 workshop</a>:Approximate Bayesian Inference: Old and New.</li>
  <li><a href="https://github.com/philschulz/VITutorial">Variational Inference and Deep Generative Models</a>:Variational Inference for NLP audiences</li>
</ul>

  </div>

  
<div class="colored-block">
  
  
    Did you like that post?
  

  You can suscribe to the
  <a href="/feed.xml">RSS feed</a>
  

  
    or follow
    <a href="https://twitter.com/sambaiga">@sambaiga</a>
    on Twitter
  .
</div>


  
  <div class="post-author">
    
      <a href="
  
">
        <img class="user-picture" src="https://github.com/sambaiga.png" alt="Anthony Faustine">
      </a>
    
      <ul class="user-info">
        <li class="user-name">
          Anthony Faustine
        </li>
        <li class="user-shortbio">
  PhD Student (IDLab Ghent).
</li>
      </ul>
  </div>
  
<script>
  renderMathInElement(
    document.getElementById("main"),
    {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "\\[", right: "\\]", display: false},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false}
      ]
    }
  );
</script>
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">
<div class="wrapper">
     Copyright 2018 
    "&copy; 2017 A.Faustine"

    
    : sambaiga@gmail.com
  </div>

  </div>
</footer>


<script src="/assets/js/katex_init.js"></script>


<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>
<script src="/assets/js/common.js"></script>


  </body>

</html>
