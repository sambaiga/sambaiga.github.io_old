<!DOCTYPE html>
<html lang="en">

  
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <title>Learning HMM parameters for Continous Density Models | 
  sambaiga
</title>
  

  
  <meta name="description" content="This post review training methods for learning standard HMM parameters with gaussian observation.
">
  
 
  
  <link rel="stylesheet" href="/assets/css/style.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  
  <link rel="canonical" href="https://sambaiga.github.io/ml/hmm/2017/06/12/hmm-gausian.html">
  <link rel="alternate" type="application/rss+xml" title="
  sambaiga
" href="/feed.xml">

  
  
  <meta name="theme-color" content="#ffffff">

  

  

  

  

  

<!--<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">

-->


<!-- The loading of KaTeX is deferred to speed up page rendering -->
 <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

<script src="https://use.fontawesome.com/3652afeffa.js"></script>
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<script src="/assets/js/common.js"></script>





</head>



  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <div class="site-header-float">
    



<a class="site-title" href="/blog/%0A">
  
    
  sambaiga

  
</a>

    

    
    <nav class="site-nav">
      <span class="menu-icon">
        <svg viewbox="0 0 18 15" width="18px" height="15px">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"></path>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"></path>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"></path>
        </svg>
      </span>

      <div class="trigger">
        

<span class="nav-list-title">Menu:</span>
<ul class="nav-list ">






  



  
  <li><a class="page-link" href="/about/">About</a></li>
  



  
  <li><a class="page-link" href="/blog/">Blog</a></li>
  



  



  



  



  
  <li><a class="page-link" href="/project/">Projects</a></li>
  



  
  <li><a class="page-link" href="/resources.html">Resources</a></li>
  



  
  <li><a class="page-link" href="/talks/">Talk</a></li>
  



  



  



  


</ul>

      </div>
    </nav>
    
    </div>

    
    

  </div>
</header>


    
    <main class="post-content" aria-label="Content">
    
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  
  <header class="post-header with-thumbnail">
  
    
    <img class="post-thumbnail" src="/assets/img/post/ghmm.jpg" alt="Learning HMM parameters for Continous Density Models">
    
    <div class="wrapper">
      <h1 class="post-title" itemprop="name headline">Learning HMM parameters for Continous Density Models</h1>
      <p class="post-meta">
        <time datetime="2017-06-12T17:12:00+02:00" itemprop="datePublished">
          

  Jun 12, 2017


        </time>
        





  
  

  
    <span class="last-update">·

    

    last updated on
    

  Oct 1, 2017



    

    </span>
  




        
      </p>
      <h3 class="post-summary">This post review training methods for learning standard HMM parameters with gaussian observation.
</h3>
    </div>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>In the previous post we considered a scenario in which observation sequences <script type="math/tex">Y</script> are discrete symbols. However, for many practical problems the observation symbols are continous vectors. As the results the contious probability desnsity function (pdfs) are used to model the space of the observation signal associated with each state. Most commonly used emission distribution are gaussian distribution and the gausian mixture models.</p>

<h3 id="gaussian-distribution-and-the-gausian-mixture-models">Gaussian Distribution and the Gausian Mixture Models</h3>

<p>It is popular to represent the randomness of continuous-valued  using the multivariate Gaussian distribution. A vector-valued random variable <script type="math/tex">\mathbf{x}</script> is said to have a multivariate normal (or Gaussian) distribution with mean <script type="math/tex">\mu=\mathop{\mathbf{E[x]}}</script> and covariance matrix <script type="math/tex">\Sigma=\mathbf{cov[x]}</script> if:</p>

<script type="math/tex; mode=display">P(\mathbf{x}; \mu, \Sigma) = \mathcal{N(\mathbf{x} \mid \mu, \Sigma)}=\frac{1}{(2\pi)^{D/2} |\Sigma|^\frac{1}{2}}\quad\exp\Big(-\frac{1}{2}[\mathbf{x} - \mu] \Sigma^{-1}[\mathbf{x} - \mu]^\mathsf{T} \Big)</script>

<p>where <script type="math/tex">D</script> is the dimensionality of <script type="math/tex">\mathbf{x}</script>. The <script type="math/tex">\mu</script> represents the location where samples are most likely to be generated and the <script type="math/tex">\Sigma</script> indicates the level to which two variables vary together.</p>

<p>However, a single Gaussian distribution is insufficient to represent the state-dependent observation space for an HMM state <script type="math/tex">s_t=i</script> because there are large amounts of training data collected from various appliance instances with different modes, distortions, background noises, etc which are used to train the parameters of individual HMM states. In this case, a Gaussian mixture model (GMM) is adopted to represent the state-dependent observation space.</p>

<p>A mixture model is a probabilistic model for density estimation using a mixture distribution and can be regarded as a type of unsupervised learning or clustering. They provide a method of describing more complex propability distributions, by combining several probability distributions. A multivariate Gaussian mixture distribution is given by the following equation:</p>

<script type="math/tex; mode=display">P(\mathbf{x}) = \displaystyle\sum_{k=1}^{K}\omega_k \mathcal{N(\mathbf{x} \mid \mu_k, \Sigma_k)}</script>

<p>The parameters <script type="math/tex">\omega_k</script> are called mixing coefficients, which must fulfill</p>

<script type="math/tex; mode=display">\displaystyle\sum_{k=1}^{K}\omega_k =1</script>

<p>and given <script type="math/tex">\mathcal{N(\mathbf{x} \mid \mu_k, \Sigma_k)} \geq 0</script> and <script type="math/tex">P(\mathbf{x}) \geq 0</script> we also have that 
<script type="math/tex">0\leq \omega_k \geq 1</script>. Each Gaussian density <script type="math/tex">\mathcal{N(\mathbf{x} \mid \mu_k, \Sigma_k)}</script> is
called a component of the mixture and has its own mean <script type="math/tex">\mu_k</script>   and covariance <script type="math/tex">\Sigma_k</script>.</p>

<h3 id="hmm-with-gaussian-emission-distribution">HMM with gaussian emission distribution</h3>

<p>If the observations are continuous, it is common for the emission probabilities to be a conditional
Gaussian such that:</p>

<script type="math/tex; mode=display">P(\mathbb{y_t} \mid s_t =i) = \mathcal{N(\mathbf{y_t} \mid \mu_i, \Sigma_i)}</script>

<p>where <script type="math/tex">\mu_i</script> and <script type="math/tex">\Sigma_i</script> are mean vector and covariance matrix associated with state <script type="math/tex">i</script>.</p>

<p>The re-estimation formula for the mean vector and covariance matrix of a state gausian pdf can be derived as:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
 \hat{\mu}_i & =\frac{\displaystyle\sum_{t=1}^{T}\gamma_t(i)\mathbb{y(t)}}{\displaystyle\sum_{t=1}^{T}\gamma _t(i)}\\
 \hat{\Sigma}_i & =\frac{\displaystyle\sum_{t=1}^{T}\gamma_t(i) [\mathbf{y(t)}-\hat{\mu}_i]\cdot[\mathbf{y(t)}-\hat{\mu}_i]^T}{\displaystyle\sum_{t=1}^{T}\gamma_t(i)}
\end{aligned} %]]></script>

<h3 id="hmms-with-gaussian-mixture-model">HMMs with Gaussian Mixture Model</h3>

<p>In HMMs with gaussian mixture pdf, the emission probabilities is given by</p>

<p><script type="math/tex">P(\mathbb{y_t} \mid s_t =i) = \displaystyle\sum_{k=1}^{M} \omega\_{ik}\mathcal{N(\mathbb{y_t} \mid \mu_{ik}, \Sigma_{ik})}</script>
  where <script type="math/tex">\omega_{ik}</script> is the prior probability of the  <script type="math/tex">k^{th}</script> component of the mixture.</p>

<p>The posterior probability of state <script type="math/tex">s_t=i</script> at time <script type="math/tex">t</script> and state <script type="math/tex">s_{t+1}=j</script> at time <script type="math/tex">t+1</script> given the model <script type="math/tex">\lambda</script> and the observation sequence <script type="math/tex">Y</script> is</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
 \gamma_t(i,j)& =P(s_t=i, s_{t+1}=j \mid Y, \lambda) \\
 & = \frac{\alpha_t(i)a_{ij}\Big[ \displaystyle\sum_{k=1}^{M} \omega_{ik}\mathcal{N(\mathbf{y_t} \mid \mu_{ik}, \Sigma_{ik})} \Big]\beta_{t+1}(j)}{\displaystyle\sum_{i=1}^{N}\alpha_T(i)}
\end{aligned} %]]></script>

<p>and the posterior probability of state <script type="math/tex">s_t=i</script> at time <script type="math/tex">t</script> given the model <script type="math/tex">\lambda</script> and observation <script type="math/tex">Y</script> is</p>

<script type="math/tex; mode=display">\gamma_t(i) =\frac{\alpha_t(i)\beta_t(i)}{\displaystyle\sum_{i=1}^{N}\alpha _T(i)}</script>

<p>Let define the joint posterior probability of the state <script type="math/tex">s_i</script> and the <script type="math/tex">k^{th}</script> gaussian component pdf of state <script type="math/tex">i</script> at time <script type="math/tex">t</script></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\xi(i,k) &= P(S_t=s_i, m(t)=k \mid Y, \lambda) \\
 &=\frac{\displaystyle\sum_{j=1}^{N} \alpha_t(j) a_{ij} \omega_{ik}\mathcal{N(\mathbf{y_t} \mid \mu_{ik}, \Sigma_{ik})}\beta_{t+1}(j)}{\displaystyle\sum_{i=1}^{N}\alpha _T(i)} 
\end{aligned} %]]></script>

<p>The re-estimation formula for the mixture coefficeints, the mean vectors and the covariance matrices of the state mixture gausian pdf as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
 \hat{\omega}_{ik} &= \frac{\displaystyle \sum_{t=1}^{T} \xi_t(i,k)}{\displaystyle\sum\_{t=0}^{T}\gamma_t(i)} \\
\hat{\mu}_{ik} &= \frac{\displaystyle\sum\ _{t=1}^{T}\xi\ _t(i,k)\mathbf{y_t}}{\displaystyle\sum_{t=1}^{T}\xi_t(i,k)} \\
\hat{\Sigma}_{ik}&=\frac{\displaystyle\sum_{t=1}^{T}\xi_t(i,k)[\mathbf{y_t}-\hat{\mu}_{ik}]\cdot[\mathbf{y_t}-\hat{\mu}_{ik}]^T}{\displaystyle\sum_{t=1}^{T}\xi_t(i,k)}
\end{aligned} %]]></script>

<h3 id="limitation-of-baumwelch-algorithm">Limitation of Baum–Welch algorithm</h3>

<p>When applying Baum–Welch algorithm  in real  data, we need to consider some heuristics in the ML EM algorithm.</p>

<ol>
  <li>How to provide initial parameters values. This is always an important question, and it is usually resolved by using a simple algorithm (e.g., K-means clustering or random initialization).</li>
  <li>How to avoid unstability in the parameter estimation (especially covariance parameter estimation) due to data sparseness. For examle some mixture components or hidden states cannot have sufficient data assigned in the Viterbi or forward–backward algorithm. This can be heuristically avoided by setting a threshold to update these parameters, or setting minimum threshold values for covariance parameters.</li>
</ol>

<p>The above two problem can be solved by the Bayesian approaches.</p>

<h3 id="references">References</h3>
<ol>
  <li>Saeed V. Vaseghi, Advanced Digital Signal Processing and Noise Reduction. John Wiley &amp; Sons, 2008.</li>
  <li>Kevin P. Murphy, Machine Learning: A Probabilistic Perspective. The MIT Press Cambridge, Massachusetts, 2012.</li>
</ol>

  </div>

  
<div class="colored-block">
  
  
    Did you like that post?
  

  You can suscribe to the
  <a href="/feed.xml">RSS feed</a>
  

  
    or follow
    <a href="https://twitter.com/sambaiga">@sambaiga</a>
    on Twitter
  .
</div>


  
  <div class="post-author">
    
      <a href="">
        <img class="user-picture" src="https://github.com/sambaiga.png" alt="Anthony Faustine">
      </a>
    
      <ul class="user-info">
        <li class="user-name">
          Anthony Faustine
        </li>
        <li class="user-shortbio">
  PhD machine learning researcher (IDLab, imec, University of Ghent).
</li>
      </ul>
  </div>
  
<script>
  renderMathInElement(
    document.getElementById("main"),
    {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "\\[", right: "\\]", display: false},
        {left: "$", right: "$", display: false},
        {left: "\\(", right: "\\)", display: false}
      ]
    }
  );
</script>
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">
<div class="wrapper">
     Copyright 2018 
    "© 2017 A.Faustine"

    
    : sambaiga@gmail.com
  </div>

  </div>
</footer>


<script src="/assets/js/katex_init.js"></script>


<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>
<script src="/assets/js/common.js"></script>

<script>
  $("script[type='math/tex']").replaceWith(function() {
      var tex = $(this).text();
      return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: false});
  });

  $("script[type='math/tex; mode=display']").replaceWith(function() {
      var tex = $(this).html();
      return katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: true});
  });
</script>




  </body>

</html>
